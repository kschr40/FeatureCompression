{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import CompressionLayer, QuantizationLayer, FeatureSelectionLayer, HardQuantizationLayer\n",
    "from models import MultiLayerPerceptron\n",
    "from datasets import get_dataloader\n",
    "from training_utils import train_model, eval_val, eval_quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'California_Housing'\n",
    "train_loader, val_loader, test_loader = get_dataloader(dataset = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_values(train_loader, num_features):\n",
    "    min_values = torch.tensor([float('inf')] * num_features)\n",
    "    max_values = torch.tensor([-float('inf')] * num_features)\n",
    "    for batch in train_loader:\n",
    "        inputs, _ = batch\n",
    "        min_values = torch.min(min_values, inputs.min(dim=0).values)\n",
    "        max_values = torch.max(max_values, inputs.max(dim=0).values)\n",
    "    return min_values, max_values\n",
    "\n",
    "min_values, max_values = get_min_max_values(train_loader, num_features=8)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8114, 1.8614, 1.0395, 0.4581, 1.6371, 0.1906, 1.5462, 1.2484])\n"
     ]
    }
   ],
   "source": [
    "def estimate_quantile(train_loader, quantile, num_features):\n",
    "    quantile_values = torch.zeros(num_features)\n",
    "    all_data = []\n",
    "\n",
    "    # Collect all data from the train_loader\n",
    "    for batch in train_loader:\n",
    "        inputs, _ = batch\n",
    "        all_data.append(inputs)\n",
    "\n",
    "    # Concatenate all data along the first dimension\n",
    "    all_data = torch.cat(all_data, dim=0)\n",
    "\n",
    "    # Compute the quantile for each feature\n",
    "    for i in range(num_features):\n",
    "        quantile_values[i] = torch.quantile(all_data[:, i], quantile)\n",
    "\n",
    "    return quantile_values\n",
    "\n",
    "# Example usage\n",
    "quantile = 0.95  # 95th percentile\n",
    "num_features = 8  # Number of features in the dataset\n",
    "quantile_values = estimate_quantile(train_loader, quantile, num_features)\n",
    "print(quantile_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = [128, 256, 512, 1024]\n",
    "max_hidden_layers = 6\n",
    "neuron_combinations = [[hidden_neuron] for hidden_neuron in hidden_neurons]\n",
    "neuron_combination_dict = {1: neuron_combinations}\n",
    "neuron_combinations = []\n",
    "for current_layers in range(2,max_hidden_layers + 1):\n",
    "    current_lists = neuron_combination_dict.get(current_layers-1)\n",
    "    new_lists = []\n",
    "    for current_list in current_lists:\n",
    "        for hidden_neuron in hidden_neurons:\n",
    "            new_list = current_list + [hidden_neuron]\n",
    "            ## Only add new_list, if it first goes up in neurons and then down\n",
    "            # if (np.diff(np.sign(np.diff(np.array(new_list)))) <= 0).all() & np.count_nonzero(np.diff(np.sign(np.diff(np.array(new_list))))) <=1:\n",
    "            ## Only add new_list, if it first goes up in neurons and then down and only has 2 values    \n",
    "            if ((np.diff(np.sign(np.diff(np.array(new_list)))) <= 0).all()) & (np.count_nonzero(np.diff(np.sign(np.diff(np.array(new_list))))) <=2) & (len(np.unique(np.array(new_list))) <= 3):\n",
    "                new_lists.append(new_list)\n",
    "    neuron_combination_dict.update({current_layers: new_lists})\n",
    "    if current_layers > 3:\n",
    "        neuron_combinations += new_lists\n",
    "\n",
    "print(len(neuron_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search_hard_quantization(min_values, max_values, n_steps = 10, n_bits =8, optimize_dict = {}, device = 'cpu', when = 'pre'):\n",
    "    # Define default hyperparameters\n",
    "    weight_decay =  0\n",
    "    learning_rate = 0.001\n",
    "    neuron_combination = [256,256]\n",
    "    num_epochs = 30\n",
    "    add_noise = False\n",
    "\n",
    "    # Lists to store results\n",
    "    random_search_losses = []\n",
    "    hyperparameter_dict = {\n",
    "        'weight_decay': [],\n",
    "        'learning_rate': [],\n",
    "        'architecture': [],\n",
    "        'num_epochs': [],\n",
    "        'add_noise': []}\n",
    "    \n",
    "\n",
    "    # Perform random search\n",
    "    for _ in tqdm(range(n_steps)):\n",
    "        for key, value in optimize_dict.items():\n",
    "            if key == 'weight_decay':\n",
    "                weight_decay = random.choice(value)\n",
    "            elif key == 'learning_rate':\n",
    "                learning_rate = random.choice(value)\n",
    "            elif key == 'neuron_combination':\n",
    "                neuron_combination = random.choice(value)\n",
    "            elif key == 'num_epochs':\n",
    "                num_epochs = random.choice(value)    \n",
    "            elif key == 'add_noise':\n",
    "                add_noise = random.choice(value)    \n",
    "            else:\n",
    "                raise ValueError(f\"Unknown hyperparameter: {key}\")\n",
    "            \n",
    "        architecture = [8] + neuron_combination + [1]\n",
    "        hyperparameter_dict['weight_decay'].append(weight_decay)\n",
    "        hyperparameter_dict['learning_rate'].append(learning_rate)\n",
    "        hyperparameter_dict['architecture'].append(neuron_combination)\n",
    "        hyperparameter_dict['num_epochs'].append(num_epochs)\n",
    "        hyperparameter_dict['add_noise'].append(add_noise)\n",
    "\n",
    "        # Create and train model\n",
    "        quantization_model = HardQuantizationLayer(n_bits=n_bits, min_values=min_values, max_values=max_values)\n",
    "        mlp = MultiLayerPerceptron(architecture)\n",
    "\n",
    "        if when == 'pre':\n",
    "            training_model = nn.Sequential(\n",
    "                quantization_model,\n",
    "                mlp\n",
    "            )\n",
    "        elif when == 'post':\n",
    "            training_model = mlp\n",
    "\n",
    "        eval_model = nn.Sequential(\n",
    "                quantization_model,\n",
    "                mlp\n",
    "            )\n",
    "        training_model.to(device)\n",
    "        eval_model.to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(training_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        \n",
    "        best_val_loss = train_model(training_model, num_epochs=num_epochs,\n",
    "                    train_loader=train_loader, test_loader=test_loader,\n",
    "                    optimizer=optimizer, criterion=criterion, has_quantization_layer=False,\n",
    "                    train_quantization_layer=False, print_result=False,\n",
    "                    add_noise=add_noise, device=device)\n",
    "\n",
    "        val_loss = eval_val(model=eval_model,\n",
    "                val_dataloader=test_loader,\n",
    "                criterion=criterion, device = device)\n",
    "        if when == 'pre':\n",
    "            random_search_losses.append(best_val_loss)\n",
    "        elif when == 'post':\n",
    "            random_search_losses.append(val_loss)    \n",
    "    # Create DataFrame with results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Architecture': hyperparameter_dict['architecture'],\n",
    "        'Loss': random_search_losses,\n",
    "        'Weight Decay': hyperparameter_dict['weight_decay'],\n",
    "        'Learning Rate': hyperparameter_dict['learning_rate'],\n",
    "        'Num Epochs': hyperparameter_dict['num_epochs'],\n",
    "        'Add Noise': hyperparameter_dict['add_noise']\n",
    "\n",
    "    })\n",
    "    results_df = results_df.sort_values('Loss')  # Sort by loss ascending    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:47<00:00, 23.81s/it]\n"
     ]
    }
   ],
   "source": [
    "results_df_pre = random_search_hard_quantization(min_values = min_values,\n",
    "                                             max_values = max_values,\n",
    "                                              n_bits = 8,\n",
    "                                              n_steps = 50,\n",
    "                                              optimize_dict=\n",
    "                                              {'weight_decay': [0, 0.0001],\n",
    "                                              'learning_rate': [0.001, 0.0001],\n",
    "                                                'add_noise': [False, True],\n",
    "                                              'neuron_combination': neuron_combinations,\n",
    "                                              'num_epochs': [30]},\n",
    "                                              device = device,\n",
    "                                              when = 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Architecture",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Loss",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Weight Decay",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Learning Rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Num Epochs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Add Noise",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "5bb7dc54-0a75-47c4-a2d9-e3e8dfbda269",
       "rows": [
        [
         "1",
         "[128, 1024, 1024, 1024, 256]",
         "0.34534161067925967",
         "0",
         "0.0001",
         "30",
         "False"
        ],
        [
         "0",
         "[128, 256, 256, 128]",
         "0.350337474162762",
         "0",
         "0.0001",
         "30",
         "True"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Weight Decay</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Num Epochs</th>\n",
       "      <th>Add Noise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[128, 1024, 1024, 1024, 256]</td>\n",
       "      <td>0.345342</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 256, 256, 128]</td>\n",
       "      <td>0.350337</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Architecture      Loss  Weight Decay  Learning Rate  \\\n",
       "1  [128, 1024, 1024, 1024, 256]  0.345342             0         0.0001   \n",
       "0          [128, 256, 256, 128]  0.350337             0         0.0001   \n",
       "\n",
       "   Num Epochs  Add Noise  \n",
       "1          30      False  \n",
       "0          30       True  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_pre.to_csv(f'results/{dataset}/random_search_results_hard_quantization_pre.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_post = random_search_hard_quantization(min_values = min_values,\n",
    "                                             max_values = max_values,\n",
    "                                              n_bits = 8,\n",
    "                                              n_steps = 50,\n",
    "                                              optimize_dict=\n",
    "                                              {'weight_decay': [0, 0.0001],\n",
    "                                              'learning_rate': [0.001, 0.0001],\n",
    "                                                'add_noise': [False, True],\n",
    "                                              'neuron_combination': neuron_combinations,\n",
    "                                              'num_epochs': [30]},\n",
    "                                              device = device,\n",
    "                                              when = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_post.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_post.to_csv(f'results/{dataset}/random_search_results_hard_quantization_post.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
